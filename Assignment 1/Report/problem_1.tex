\section*{Problem 1}
It is assumed throughout this problem that whenever a matrix $A$ is referenced, it is a real and symmetric matrix of size $n \times n$.
\\

\textbf{Part (i):} \textit{Prove that a real symmetric matrix has real eigenvalues}

Let $\lambda$ be an eigenvalue to $A$ with a corresponding vector $v$, which is by the definition of an eigenvector non-zero.

The norm of a complex vector $z$ is given by $ \lVert z \rVert = \sqrt{z^T \overline{z}}$ and is non-zero for all $z \in \mathbb{C}$ except $z = 0$.

Given that $A$ is real we can use that

\begin{equation}
  \overline{Av} = \overline{\lambda v} = A \overline{v} = \overline{\lambda v}
  \label{real_matrix}
\end{equation}

Then we can expand $v^T A \overline{v}$ in the two following ways

\begin{equation}
  v^T A \overline{v} = v^T (A \overline{v}) = \bigg\{ \text{Using equation (\ref{real_matrix})}\bigg\} = v^T \overline{\lambda} \overline{v} =  \overline{\lambda} v^T  \overline{v} = \overline{\lambda} \lVert v \rVert ^2
  \label{i_1}
\end{equation}

\begin{equation}
  v^T A \overline{v} = (A^T v)^T \overline{v} = \bigg\{A = A^T \bigg\} = (A v)^T \overline{v} = \lambda v^T \overline{v} = \lambda \lVert v \rVert ^2
  \label{i_2}
\end{equation}

Thus since equation (\ref{i_1}) and (\ref{i_2}) are equal (from the left hand side) we get that

\begin{equation}
  \overline{\lambda} \lVert v \rVert^2= \lambda \lVert v \rVert^2 \Rightarrow \lambda = \overline{\lambda}
\end{equation}

 since $v$ is non-zero by the definition of an eigenvector. Thus are the eigenvalues of a real and symmetric matrix real.
\\

\textbf{Part (ii):} \textit{Prove that a real symmetric matrix has orthogonal eigenvectors. Then, prove that the eigen-decomposition of a real symmetric matrix $A$ is $A = Q\Lambda Q^T$}

Let $A \in \mathbb{R^{n \times n}}$ be a symmetric matrix. I will show this by using a proof of induction. The definition of an eigenvector $v$ with an eigenvalue $\lambda$ is that they fulfill the characteristic equation

\begin{align*}
  (A-\lambda I)v = 0, \; v \text{ is non-trivial}
\end{align*}

which implies that the kernel of $(A-\lambda I)$ is non-zero which in turn implies that

\begin{align*}
  det(A-\lambda I) = 0
\end{align*}
which is a polynomial equation of degree $n$ with at least one root being $\lambda \in \mathbb{R}$ (because of the symmetric properties of A). Thus is $Av = \lambda v$ and we have at least one eigenvector $v$ which is trivially orthogonal and the base case is proven.

Now; assuming that we have $n-m$ orthogonal eigenvectors of $A$
$$ \; v_1, v_2, ..., v_{n-m}, \; m < n$$
we want to prove that we have at least one more eigenvector.

Let the vectors $u_1, u_2, ..., u_m$ be orthogonal to both each other and the vectors $v_1, v_2, ..., v_{n-m}$ (which is always possible as we have $n$ vectors in $\mathbb{R}^n$). Additionally, let $U \in \mathbb{R}^{n \times m}$ be the matrix with $u_1, u_2, ..., u_m$ as columns.

Then the matrix $Y = U^T A U$ is a symmetric matrix as

\begin{align*}
  Y = U^T A U = (U^T A^T U)^T = \bigg \{ A = A^T \bigg \} = (U^T A U)^T
\end{align*}

Thus, by the same arguments made previously in the base case does $Y$ have at least one eigenvector $w$ with a corresponding eigenvalue $\mu \in \mathbb{R}$ and the following holds

\begin{align}
  & Yw = U^T A Uw = \mu w \\
  & \Rightarrow U U^T A Uw = U \mu w
  \label{ort_1}
\end{align}

Can now let $v_{n-m+1} = Uw$ which is a linear combination of the vectors in $U$ which implies that $v_{n-m+1}$ is orthogonal to $v_1, v_2, ..., v_{n-m}$ since all vectors $u_1, u_2, ..., u_m$ are so. Substituting this into equation (\ref{ort_1}) yields

\begin{align}
  & \Rightarrow U U^T A Uw = U \mu w \\
  & \Rightarrow U U^T A v_{n-m+1} = \mu v_{n-m+1}
  \label{ort_2}
\end{align}

We can now use that $v_{n-m+1}$ is orthogonal to $v_1, v_2, ..., v_{n-m}$ and show that $Av_{n-m+1}$ is also orthogonal to $v_1, v_2, ..., v_{n-m}$ by

\begin{align}
  (Av_{n-m+1})^T v_i & = v_{n-m+1}^T A^T v_i =  \bigg \{ A = A^T \bigg \} = v_{n-m+1}^T (A v_i)\\
  & = \lambda_i v_{n-m+1}^T v_i = 0
\end{align}

and thus is $Av_{n-m+1}$ also orthogonal to $v_1, v_2, ..., v_{n-m}$ which means that $Av_{n-m+1}$ is also a linear combination of $u_1, u_2, ..., u_m$ and can be written as $Av = Ux$. Substituting this into equation (\ref{ort_2}) yields

\begin{align}
  U U^T A v_{n-m+1} = U U^T U x = Ux = Av_{n-m+1} = \mu v_{n-m+1}
\end{align}

and thus is $v_{n-m+1}$ an eigenvector of $A$ which which completes the induction and we have proven that a real symmetric matrix has $n$ orthogonal eigenvectors.

Now we just need to prove that it can be decomposed as $A = V \Lambda V^T$. First we will show that it can assumed that all eigenvectors are normalised without loss of generality since

\begin{align*}
  Av = \lambda v \Rightarrow A \frac{v}{\| v \|} = \lambda \frac{v}{\| v \|}
\end{align*}

Now suppose that $v_1, v_2, ..., v_{n}$ are orthonormal eigenvectors of $A$ with corresponding eigenvalues $\lambda_1, \lambda_2,..., \lambda_n$ and let $V$ be the matrix with $v_1, v_2, ..., v_{n}$ as columns. Then the product

\begin{align*}
  (V^TV)_{ij} = v_i^Tv_j =
  \begin{cases}
    1, & i = j\\
    0,              & \text{otherwise}
  \end{cases}
\end{align*}

which yields that

\begin{align*}
  V^TV = I \Rightarrow V^T V V^{-1} = V^{-1} \Rightarrow V^T = V^{-1}
\end{align*}

which implies that $V$ is orthogonal.

Letting $\Lambda = \text{diag}(\lambda_1, \lambda_2,..., \lambda_n)$ we want to show that $A = V \Lambda V^T$. Since $v_1, v_2, ..., v_{n}$ are orthonormal any vector $x \in \mathbb{R}^n$ can be expressed as

\begin{align*}
  x = \sum_{i=1}^n \alpha_i v_i
\end{align*}
Then

\begin{align*}
  Ax = A  \sum_{i=1}^n \alpha_i v_i =  \sum_{i=1}^n \alpha_i Av_i =  \sum_{i=1}^n \alpha_i \lambda_i v_i
\end{align*}
and

\begin{align*}
  V \Lambda V^T x & = V \Lambda V^T \sum_{i=1}^n \alpha_i v_i = V \Lambda \sum_{i=1}^n \alpha_i e_i \\
  & = V \sum_{i=1}^n \alpha_i \lambda_i e_i = \sum_{i=1}^n \alpha_i \lambda_i v_i
\end{align*}
which yields that

\begin{align}
  Ax = \sum_{i=1}^n \alpha_i \lambda_i v_i = V \Lambda V^T x \Rightarrow A = V \Lambda V^T
\end{align}
which was to be proven. 

\textbf{Part (iii):} \textit{Prove that a positive semi definite matrix has non negative eigenvalues}

Let $B$ be a complex $n \times n$ positive semi-definite matrix accompanied by an eigenvalue $\lambda$ and an associated eigenvector $v$. By the definition of a positive semi-definite matrix we know that for any vector $z \in \mathbb{C}$ it holds that $\overline{z}^T B z \geq 0$. The following thus holds

\begin{equation}
  \overline{v}^T B v = \overline{v}^T \lambda v = \lambda \overline{v}^T v = \lambda \lVert v \rVert^2 \geq 0
  \label{iii_1}
\end{equation}

where $\lVert v \rVert^2$ is necessarily positive since eigenvectors are by definition non-zero. This implies that $\lambda \geq 0$, which was to be proven.
\\

\textbf{Part (iv):} \textit{Let $A \in \mathbb{R}^{n \times n}$ symmetric and positive semi-definite matrix. Define a matrix $D = \{ D | D_{ij} = A_{ii} + A_{jj} - 2A_{ij} \}$. Show that there exists $n$ vectors $v_1, ..., v_n, \; v_i \in \mathbb{R}^n \; \forall i$} such that $D_{ij} = \lVert v_i - v_j \rVert^2_2$.

\begin{equation}
  D_{ij} = \lVert v_i - v_j \rVert^2_2 = v_i^T v_i + v_j^T v_j - 2v_i^T v_j
\end{equation}

Thus, if we can show that any matrix $A \in \mathbb{R}^{n \times n}$ that fulfils the given conditions, can have each of its elements expressed as a dot product between $n$ given vectors $v_1, ..., v_n, \; v_i \in \mathbb{R}^n \; \forall i$ we have shown what is asked.

Given that $A$ is symmetric and positive semi-definite it can be decomposed into the following eigen-decomposition $A = Q \Lambda Q^T$ where $\Lambda$ is a diagonal matrix of real eigenvalues (since A is PSD) and $Q$ is an orthogonal matrix of eigenvectors of $A$. The decomposition can then be rewritten in the following manner

\begin{equation}
  A = Q \Lambda Q^T = (\Lambda^{\frac{1}{2}} Q^T)^T (\Lambda^{\frac{1}{2}} Q^T) = V^T V
\end{equation}

If we thus choose every $v_i, \; i = 1 ,..., n$ to be the $i$:th column in the matrix $(\Lambda^{-\frac{1}{2}} Q^T)$ we get that $A_{ij} = (V^T V)_{ij} = v_i^T v_j$ which yields the final result

\begin{equation}
  D_{ij} = A_{ii} + A_{jj} - 2A_{ij} = v_i^T v_i + v_j^T v_j - 2v_i^T v_j = \lVert v_i - v_j \rVert^2_2
\end{equation}

which was to be proven.
